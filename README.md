# Flexible neural networks toolbox (FNNT)

Main purpose of this toolbox is to be flexible enough to do ANN architectures related experiments.

So far it supports only basic building blocks for a network.
Yet, it is still possible to achieve up to 1.41% error rate on MNIST.

Some of the features:
* RELU, tansig and softmax activation functions
* MSE and crossentropy loss functions
* classic momentum
* L1 and L2 regularizations
* dropout
* data normalization
* batch mode training using gradient descent
* monitoring of training status

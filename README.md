# Flexible neural networks toolbox (FNNT)

Main purpose of this toolbox is to be flexible enough to do ANN architectures related experiments.

So far it supports only basic building blocks for a network.
Yet, it is still possible to train a network with "decent" accuracy on MNIST (less than 2% errors).

Some of the features:
* RELU, Tansig and Softmax activation functions
* Momentum
* L1 and L2 regularizations
* Dropout
